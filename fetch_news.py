import requests
import os
import urllib.parse
from datetime import datetime, timedelta

MARKETAUX_KEY = os.getenv("MARKETAUX_API_KEY")
WECHAT_WEBHOOK = os.getenv("WECHAT_WEBHOOK")

# Ê†∏ÂøÉÁôΩÂêçÂçï‰ø°Ê∫ê
WHITELIST_SOURCES = ["Bloomberg", "Reuters", "The Wall Street Journal", "CNBC", "Financial Times", "MarketWatch", "Forbes"]

def google_translate(text):
    """Âº∫Âà∂‰ΩøÁî® Google ÁøªËØëÈïúÂÉè"""
    try:
        encoded_text = urllib.parse.quote(text[:400])
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=zh-CN&dt=t&q={encoded_text}"
        r = requests.get(url, timeout=10)
        return "".join([s[0] for s in r.json()[0]])
    except:
        return text

def fetch_data(params):
    """ÈÄöÁî®ÊäìÂèñÂáΩÊï∞"""
    base_url = "https://api.marketaux.com/v1/news/all"
    params.update({"api_token": MARKETAUX_KEY, "language": "en", "limit": 10})
    try:
        res = requests.get(base_url, params=params, timeout=15).json()
        return res.get('data', [])
    except:
        return []

def get_integrated_report():
    # 1. Ëé∑ÂèñÂÆèËßÇÈáëËûç (ÁæéÂÄ∫, ÈªÑÈáë, ÊåáÊï∞, Ê¨ßÊó•Â∏ÇÂú∫)
    macro_params = {
        "entity_types": "index,commodity,currency",
        "published_after": (datetime.now() - timedelta(hours=24)).strftime('%Y-%m-%dT%H:%M')
    }
    
    # 2. Ëé∑ÂèñÂâçÊ≤øÁßëÊäÄ (AI, Ëà™Á©∫Ëà™Â§©, Web3)
    tech_params = {
        "search": "AI,Aerospace,Web3,SpaceX,NVIDIA,OpenAI",
        "industries": "Technology,Industrials"
    }

    raw_news = fetch_data(macro_params) + fetch_data(tech_params)
    
    # Á≠õÈÄâ‰∏éÂéªÈáç
    final_items = []
    seen_titles = set()
    
    for item in raw_news:
        title = item.get('title', '')
        source = item.get('source', '')
        
        # ‰ªÖ‰øùÁïôÁôΩÂêçÂçï‰ø°Ê∫êÊàñÊûÅÈ´òË¥®ÈáèÊ∫ê
        is_pro_source = any(ws in source for ws in WHITELIST_SOURCES)
        
        if title not in seen_titles and is_pro_source:
            zh_title = google_translate(title)
            
            # ËΩ¨Êç¢Êó∂Èó¥
            pub_at = item.get('published_at', '')
            time_str = "NEW"
            if pub_at:
                dt = datetime.strptime(pub_at, '%Y-%m-%dT%H:%M:%S.%fZ') + timedelta(hours=8)
                time_str = dt.strftime('%H:%M')
            
            final_items.append({
                "time": time_str,
                "source": source,
                "title": zh_title
            })
            seen_titles.add(title)

    if not final_items:
        return "### üåê È°∂Á∫ßË¥¢ÁªèÂÜÖÂèÇ\n> ÁõëÊµã‰∏≠ÔºöÊöÇÊó†Êù•Ëá™ WSJ/Bloomberg ÁöÑÂÆûÊó∂Ê†∏ÂøÉÂø´ËÆØ„ÄÇ"

    # ÊûÑÂª∫ÊéíÁâà
    now_bj = (datetime.now() + timedelta(hours=8)).strftime('%m-%d %H:%M')
    content = f"### üåê È°∂Á∫ßË¥¢ÁªèÂÜÖÂèÇ (ÂçéÂ∞îË°ó‰∏ìÁ∫ø)\n> Ë¶ÜÁõñÔºöÂÆèËßÇÈáëËûç | AI | Ëà™Â§© | Web3\n> Êõ¥Êñ∞Êó∂Èó¥Ôºö{now_bj}\n\n"
    
    for i, news in enumerate(final_items[:12], 1): # ÂèñÂâç12Êù°Á≤æÂçé
        content += f"{i}. **[{news['time']}]** {news['title']}\n   *‰ø°Ê∫ê: {news['source']}*\n\n"
    
    content += "---\n> ‚ö° ‰ªÖÊé®ÈÄÅ Bloomberg/Reuters/WSJ Á≠â‰∏ì‰∏ö‰ø°Ê∫ê"
    return content

if __name__ == "__main__":
    report = get_integrated_report()
    if WECHAT_WEBHOOK:
        requests.post(WECHAT_WEBHOOK, json={"msgtype": "markdown", "markdown": {"content": report}})
